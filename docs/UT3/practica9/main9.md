---
title: "Codificando la realidad: c√≥mo el encoding categ√≥rico mejora la predicci√≥n de ingresos en datos del censo"
date: 2025-10-15
---

## Contexto  
Esta pr√°ctica aborda el proceso de **codificaci√≥n avanzada de variables categ√≥ricas** dentro del flujo de *machine learning*, utilizando el cl√°sico **Adult Income Dataset** del Censo de Estados Unidos (1994), con m√°s de 48.000 registros.  
El objetivo del caso es **predecir si una persona percibe ingresos superiores a 50.000 d√≥lares anuales**, a partir de variables demogr√°ficas y laborales que incluyen educaci√≥n, ocupaci√≥n, pa√≠s de origen y estado civil.  

El desaf√≠o central radica en la **alta cardinalidad** de ciertas variables categ√≥ricas (como `occupation` y `native-country`), que impiden el uso directo de t√©cnicas simples como *one-hot encoding* sin incrementar dr√°sticamente la dimensionalidad del conjunto de datos.  
Por ello, se implementan y comparan m√∫ltiples estrategias de codificaci√≥n ‚Äî*Label Encoding*, *One-Hot Encoding* y *Target Encoding*‚Äî evaluando su impacto en la **precisi√≥n, interpretabilidad y eficiencia del modelo de clasificaci√≥n**.  

La pr√°ctica se desarrolla bajo la metodolog√≠a **CRISP-DM**, con especial √©nfasis en la fase de *Data Preparation* y su influencia directa sobre los resultados del modelo predictivo.

---

## Objetivos  
- Analizar la **cardinalidad** de las variables categ√≥ricas y clasificarlas seg√∫n su complejidad.  
- Implementar y comparar m√∫ltiples t√©cnicas de **encoding** (Label, One-Hot, Target).  
- Prevenir **data leakage** mediante el uso de *cross-validation* en el *target encoding*.  
- Dise√±ar **pipelines escalables** utilizando `ColumnTransformer` y *branching*.  
- Evaluar el impacto de cada m√©todo sobre la **precisi√≥n, dimensionalidad y tiempo de entrenamiento**.  
- Documentar los resultados y reflexionar sobre los **trade-offs entre interpretabilidad y performance**.

---

## Actividades  

1. **Instalaci√≥n de dependencias y configuraci√≥n del entorno**  
   - Configuraci√≥n de Google Colab e instalaci√≥n de librer√≠as (`pandas`, `numpy`, `seaborn`, `category_encoders`, `scikit-learn`, `shap`).  

2. **Carga y exploraci√≥n del dataset real ‚Äì Adult Income (Census)**  
   - An√°lisis inicial del conjunto de datos y revisi√≥n de las variables categ√≥ricas.  
   - Identificaci√≥n de variables con **alta cardinalidad** (`occupation`: 15 valores, `native-country`: 42 valores).  

3. **Experimento 1 ‚Äì Label Encoding**  
   - Aplicaci√≥n del *LabelEncoder* sobre variables categ√≥ricas.  
   - Evaluaci√≥n del modelo de clasificaci√≥n (*Logistic Regression*) y registro de m√©tricas de precisi√≥n y recall.  
   - Observaci√≥n: p√©rdida de interpretabilidad y posible introducci√≥n de relaciones ordinales artificiales.  

4. **Experimento 2 ‚Äì One-Hot Encoding (baja cardinalidad)**  
   - Implementaci√≥n del *OneHotEncoder* sobre variables con menos de 10 categor√≠as.  
   - Reducci√≥n parcial de dimensionalidad mediante *ColumnTransformer*.  
   - Ventaja: alta interpretabilidad. Desventaja: incremento significativo en el n√∫mero de columnas.  

5. **Experimento 3 ‚Äì Target Encoding (alta cardinalidad)**  
   - Aplicaci√≥n de *TargetEncoder* de `category_encoders`, con validaci√≥n cruzada para evitar *data leakage*.  
   - C√°lculo de suavizado (*smoothing*) para mitigar el sobreajuste.  
   - Resultados: mejora en la precisi√≥n sin penalizar la eficiencia del modelo.  

6. **Pipeline con Branching ‚Äì ColumnTransformer**  
   - Construcci√≥n de un *pipeline h√≠brido* combinando las tres estrategias de encoding seg√∫n la cardinalidad.  
   - Uso de *branching* para aplicar t√©cnicas diferentes en paralelo dentro del mismo flujo de entrenamiento.  

7. **Explicabilidad ‚Äì Feature Importance y SHAP**  
   - Estimaci√≥n de la importancia de variables con `RandomForestClassifier`.  
   - Interpretaci√≥n de contribuciones locales mediante *SHAP values*, comprobando el peso relativo de variables transformadas.  

8. **Comparaci√≥n de resultados y trade-offs**  
   - Comparaci√≥n de m√©tricas entre m√©todos: precisi√≥n, F1-score y tiempo de entrenamiento.  
   - Discusi√≥n de los *trade-offs* entre complejidad, interpretabilidad y rendimiento.  

9. **Investigaci√≥n libre ‚Äì T√©cnicas adicionales**  
   - Implementaci√≥n de *Frequency Encoding* y *Ordinal Encoding* como variantes complementarias.  
   - An√°lisis del impacto marginal de estas t√©cnicas en modelos de √°rboles y regresi√≥n log√≠stica.  

---

## Desarrollo  

El desarrollo sigui√≥ un enfoque experimental progresivo basado en la comparaci√≥n emp√≠rica de m√©todos de codificaci√≥n.  
El trabajo comenz√≥ con la **exploraci√≥n de cardinalidad** y el an√°lisis de distribuci√≥n de categor√≠as, lo que permiti√≥ clasificar las variables seg√∫n su nivel de complejidad y definir una estrategia diferenciada para cada grupo.  

A partir de all√≠, se dise√±aron tres experimentos independientes (Label, One-Hot y Target Encoding) bajo un esquema controlado, utilizando las mismas m√©tricas, modelo base y divisi√≥n de datos para asegurar una comparaci√≥n justa.  
Cada pipeline fue implementado dentro de un `ColumnTransformer`, lo que permiti√≥ **modularizar las transformaciones** y mantener un flujo reproducible.  

En el caso del *Target Encoding*, se aplic√≥ validaci√≥n cruzada (*KFold*) para prevenir fugas de informaci√≥n entre conjuntos de entrenamiento y validaci√≥n.  
El par√°metro de *smoothing* se calibr√≥ para equilibrar la estabilidad de las medias y la variabilidad de las categor√≠as menos frecuentes.  

Finalmente, los resultados de cada t√©cnica fueron integrados y analizados con herramientas de interpretabilidad (*feature importance* y *SHAP*), lo que permiti√≥ cuantificar el efecto del encoding sobre la relevancia y comportamiento de las variables dentro del modelo.  
El pipeline resultante demostr√≥ la importancia de **elegir la t√©cnica adecuada seg√∫n la naturaleza de cada variable**, optimizando as√≠ la precisi√≥n sin comprometer la escalabilidad ni la claridad del modelo.

---

## Evidencias  
## üìä Evidencias Visuales

### üîπ Cardinalidad de Variables Categ√≥ricas
![Cardinalidad de Variables Categ√≥ricas](IMG_4254.png)  
Se visualiza la cantidad de categor√≠as √∫nicas por variable, base para definir qu√© variables se tratan con One-Hot Encoding (baja cardinalidad) y cu√°les requieren Target Encoding (alta cardinalidad).

### üîπ Distribuci√≥n de Education seg√∫n nivel de ingreso
![Distribuci√≥n Education](IMG_4326.png)  
Permite observar la relaci√≥n entre nivel educativo y probabilidad de superar los $50K, destacando la diferencia entre niveles secundarios y universitarios.

### üîπ Distribuci√≥n de Occupation seg√∫n nivel de ingreso
![Distribuci√≥n Occupation](IMG_4261.png)  
Muestra las ocupaciones m√°s asociadas con ingresos altos, destacando la brecha entre categor√≠as profesionales y de servicio.

### üîπ Distribuci√≥n de Relationship seg√∫n nivel de ingreso
![Distribuci√≥n Relationship](IMG_4262.png)  
Comparaci√≥n de tipos de relaci√≥n familiar frente al nivel de ingreso, evidenciando patrones significativos en el grupo ‚ÄúHusband‚Äù.

### üîπ Matriz de Correlaciones ‚Äì Variables num√©ricas y target
![Matriz de Correlaciones Num√©ricas](IMG_4258.png)  
Correlaciones entre variables num√©ricas y la variable objetivo; se destacan `education-num`, `age` y `capital-gain` como las m√°s relevantes.

### üîπ Matriz de Correlaciones (Top variables por relaci√≥n con el target)
![Matriz de Correlaciones Top](IMG_4328.png)  
Refina el an√°lisis mostrando solo las variables con mayor peso predictivo sobre el target, √∫til para priorizar en la etapa de feature selection.

### üîπ Importancia de Features ‚Äì Modelo Final
![Top 15 Features m√°s importantes](IMG_4257.png)  
Ranking de las 15 variables m√°s influyentes seg√∫n el modelo final (Random Forest con pipeline), se√±alando la relevancia de `fnlwgt`, `age` y `education-num`.

### üîπ Importancia y Distribuci√≥n de Features
![Distribuci√≥n Importancia de Features](IMG_4320.png)  
Comparaci√≥n entre la importancia media y la dispersi√≥n de variables en el modelo, identificando las que aportan m√°s valor explicativo.

### üîπ Comparaci√≥n de Modelos y Codificaci√≥n
![Comparaci√≥n de Modelos y Encoding](IMG_4252.png)  
Comparaci√≥n global entre Label, One-Hot, Target y Pipeline mixto en m√©tricas de Accuracy, AUC-ROC, F1, tiempo de entrenamiento y dimensionalidad.  
Evidencia que el enfoque mixto logra el mejor equilibrio entre precisi√≥n y eficiencia.

### Interpretaci√≥n General

El an√°lisis visual permite comprender c√≥mo las variables num√©ricas y categ√≥ricas influyen en el nivel de ingresos de las personas seg√∫n el Censo de EE.UU. (1994).  
Se observa que **edad, educaci√≥n y capital-gain** son los predictores m√°s relevantes, mientras que las variables categ√≥ricas como **ocupaci√≥n, estado civil y relaci√≥n familiar** tambi√©n aportan poder explicativo significativo tras aplicar t√©cnicas de codificaci√≥n adecuadas.

El uso combinado de **One-Hot Encoding** para variables de baja cardinalidad y **Target Encoding** para las de alta cardinalidad permiti√≥ reducir la dimensionalidad sin perder desempe√±o.  
El modelo final logr√≥ un equilibrio entre **precisi√≥n y eficiencia**, demostrando la importancia de un dise√±o de pipeline estrat√©gico para datos reales con gran diversidad de categor√≠as.

---

## Insights clave  

- El *feature encoding* es una etapa determinante para la **eficiencia y generalizaci√≥n** del modelo.  
- El *Label Encoding* resulta adecuado solo para algoritmos basados en √°rboles, donde la jerarqu√≠a num√©rica no induce sesgos.  
- El *One-Hot Encoding* mantiene interpretabilidad, pero **penaliza la dimensionalidad**, especialmente en variables con alta cardinalidad.  
- El *Target Encoding*, correctamente implementado con *cross-validation*, logra **mejor precisi√≥n y estabilidad**, evitando fugas de informaci√≥n.  
- Los *pipelines h√≠bridos* permiten integrar m√∫ltiples m√©todos seg√∫n la naturaleza de las variables, optimizando la relaci√≥n entre complejidad y desempe√±o.  
- Las m√©tricas obtenidas y el an√°lisis con *SHAP* demostraron que la codificaci√≥n influye directamente en la **importancia relativa de las variables** y en la transparencia del modelo.

---

## Reflexi√≥n  

Esta pr√°ctica permiti√≥ comprender el **rol cr√≠tico del encoding categ√≥rico** como puente entre los datos reales y la capacidad predictiva de los modelos.  
El proceso evidenci√≥ que la calidad de la representaci√≥n de las variables categ√≥ricas **determina en gran medida la precisi√≥n, la interpretabilidad y la eficiencia computacional** del modelo final.  

La comparaci√≥n de t√©cnicas mostr√≥ que no existe un m√©todo universalmente superior: cada enfoque presenta **trade-offs espec√≠ficos** entre complejidad, escalabilidad y robustez.  
El *Target Encoding* se destac√≥ por su balance entre poder predictivo y control de dimensionalidad, siempre que se aplique con mecanismos de validaci√≥n que eviten *data leakage*.  

Desde una perspectiva de ingenier√≠a de datos, la pr√°ctica reafirma que **las decisiones tomadas en la etapa de preparaci√≥n son tan importantes como la elecci√≥n del modelo en s√≠**, y que un pipeline bien dise√±ado puede marcar la diferencia entre un modelo funcional y uno verdaderamente confiable.  

En s√≠ntesis, el *feature encoding* se consolida como una fase estrat√©gica del proceso anal√≠tico: **transforma la informaci√≥n categ√≥rica del mundo real en representaciones matem√°ticas que los algoritmos pueden entender, sin perder su significado original.**

---

## Notebook en Google Colab  
üìì El notebook completo con el desarrollo de esta pr√°ctica puede consultarse en el siguiente enlace:  
[Abrir en Google Colab](https://colab.research.google.com/github/Agustina-Esquibel/Ingenieria-datos/blob/main/docs/UT3/practica9/UT3_Practica9.ipynb)

---

## üîó Referencias oficiales  
- [Category Encoders Library](https://contrib.scikit-learn.org/category_encoders/)  
- [Scikit-learn Preprocessing Documentation](https://scikit-learn.org/stable/modules/preprocessing.html)  
- [Feature Engineering for Machine Learning ‚Äì O‚ÄôReilly, Cap. 5](https://www.oreilly.com/library/view/hands-on-machine-learning/9781098125967/)  
- [Kaggle Feature Engineering ‚Äì Target Encoding Guide](https://www.kaggle.com/code/ryanholbrook/target-encoding)  

---

## Navegaci√≥n  
‚¨ÖÔ∏è [Volver a Pr√°ctica 8](../practica8/main8.md)  
‚û°Ô∏è [Ir a Pr√°ctica 10](../practica10/main10.md)  
üìì [√çndice del Portafolio](../../portfolio/index.md)
