---
title: "Reduciendo el ruido: c√≥mo PCA y Feature Selection revelan las variables clave del valor inmobiliario"
date: 2025-11-12
---

## Contexto
Esta pr√°ctica aborda el desaf√≠o de **reducir la complejidad de los datos sin perder su esencia predictiva**, aplicando dos estrategias complementarias:  
**PCA (An√°lisis de Componentes Principales)** y **Feature Selection**.  

A partir del dataset **Ames Housing**, se busca identificar **qu√© variables explican realmente el precio de una vivienda**, diferenciando entre ruido estad√≠stico y se√±ales valiosas.  
El objetivo es descubrir c√≥mo distintas t√©cnicas ‚Äîdesde transformaciones lineales hasta selecci√≥n basada en modelos‚Äî pueden mejorar la precisi√≥n del modelo y, al mismo tiempo, hacerlo m√°s interpretable y eficiente.  

El enfoque combina rigor t√©cnico con una mirada de negocio, mostrando que **reducir variables no significa perder informaci√≥n**, sino concentrar la atenci√≥n en los factores que m√°s impacto tienen en el mercado inmobiliario.

---

## Objetivos  
- Aplicar **PCA** para reducir dimensionalidad y analizar la varianza explicada.  
- Evaluar distintos m√©todos de **Feature Selection** (Filter, Wrapper, Embedded).  
- Comparar el impacto de cada m√©todo sobre las m√©tricas de rendimiento (RMSE y R¬≤).  
- Identificar las **features m√°s robustas** y su relevancia en la predicci√≥n del precio de venta.  
- Reflexionar sobre los **trade-offs** entre precisi√≥n, interpretabilidad y costo computacional.

---

## Actividades  

1. **Preparaci√≥n y preprocesamiento de datos**  
   - Carga y limpieza del dataset *Ames Housing* (1.460 observaciones, ~80 features).  
   - Imputaci√≥n de valores faltantes (`median` y `most_frequent`).  
   - Codificaci√≥n de variables categ√≥ricas con *Label Encoding*.  
   - Escalado de variables mediante `StandardScaler`.  

2. **Aplicaci√≥n de PCA (An√°lisis de Componentes Principales)**  
   - Estandarizaci√≥n de datos + PCA con distintos valores de `n_components`.  
   - An√°lisis de la **varianza explicada acumulada** y generaci√≥n de *scree plots*.  
   - Selecci√≥n de componentes por umbral de varianza (80%, 90%, 95%).  
   - Interpretaci√≥n de los *loadings* para entender qu√© combinaciones de features explican mejor la varianza del precio.  

3. **Filter Methods**  
   - Implementaci√≥n de `SelectKBest` con **F-test (f_regression)** y **Mutual Information (mutual_info_regression)**.  
   - Selecci√≥n de top-k features (k = 20, 30, 40) y comparaci√≥n de desempe√±o con *Random Forest Regressor*.  
   - Identificaci√≥n de features m√°s influyentes seg√∫n correlaci√≥n y relevancia estad√≠stica.  

4. **Wrapper Methods**  
   - Aplicaci√≥n de **Forward Selection**, **Backward Elimination** y **Recursive Feature Elimination (RFE)**.  
   - Evaluaci√≥n de precisi√≥n mediante *cross-validation* (5-fold).  
   - Comparaci√≥n de tiempos de ejecuci√≥n y n√∫mero de features seleccionadas.  

5. **Embedded Methods**  
   - Entrenamiento de modelos con **LassoCV**, **RidgeCV** y **Random Forest**.  
   - An√°lisis de coeficientes no nulos (Lasso) y *feature importance* (Random Forest).  
   - Visualizaci√≥n de las 10 variables m√°s relevantes seg√∫n importancia del modelo.  

6. **Comparaci√≥n global de resultados**  
   - Consolidaci√≥n de resultados en una tabla resumen de RMSE, R¬≤ y cantidad de features.  
   - Detecci√≥n de *features consistentes* (presentes en m√∫ltiples m√©todos).  
   - Discusi√≥n sobre el equilibrio entre performance, interpretabilidad y tiempo de c√≥mputo.  

---

## Desarrollo  

El desarrollo se realiz√≥ de manera incremental, iniciando con el preprocesamiento y escalado del dataset.  
Posteriormente, se aplicaron los tres enfoques principales de reducci√≥n de dimensionalidad:

1. **Filter Methods:** r√°pidos y estad√≠sticamente interpretables.  
2. **Wrapper Methods:** m√°s precisos pero de alto costo computacional.  
3. **Embedded Methods:** integran la selecci√≥n de variables en el proceso de entrenamiento del modelo.  

Cada m√©todo fue evaluado mediante validaci√≥n cruzada (5-fold) sobre m√©tricas de **RMSE** y **R¬≤**, considerando adem√°s la cantidad de features seleccionadas y el tiempo de ejecuci√≥n.

Los resultados mostraron que el m√©todo **Mutual Information (Filter)** ofreci√≥ el mejor balance entre velocidad y precisi√≥n, mientras que **Forward Selection (Wrapper)** alcanz√≥ un RMSE similar con menos variables, aunque con mayor tiempo de c√≥mputo.  
El modelo **Random Forest (Embedded)** se destac√≥ por su estabilidad y facilidad para interpretar la importancia relativa de las variables.

---

## Evidencias  

---

## Comparaci√≥n entre PCA y Feature Selection

| Criterio | PCA (An√°lisis de Componentes Principales) | Feature Selection (Filter, Wrapper, Embedded) |
|---------|--------------------------------------------|------------------------------------------------|
| **Prop√≥sito principal** | Reducir dimensionalidad mediante combinaciones lineales de las variables originales. | Seleccionar las variables m√°s relevantes bas√°ndose en m√©tricas estad√≠sticas o informaci√≥n del modelo. |
| **Interpretabilidad** | Baja: los componentes no representan variables reales sino combinaciones. | Alta: las variables seleccionadas conservan significado de negocio. |
| **Manejo de colinealidad** | Excelente: condensa informaci√≥n redundante en pocos componentes. | Bueno: algunos m√©todos (Lasso, RF, MI) detectan redundancia, pero no siempre la resuelven completamente. |
| **Impacto en performance del modelo** | Mejora rendimiento cuando el dataset tiene mucha correlaci√≥n interna o ruido. | Mejora rendimiento al eliminar variables irrelevantes sin perder interpretabilidad. |
| **Costo computacional** | Bajo a moderado. PCA es r√°pido una vez escalado el dataset. | Variable: Filter es r√°pido; Wrapper puede ser muy costoso; Embedded intermedio. |
| **Requerimiento de escalado** | Obligatorio (`StandardScaler`). | No siempre necesario (depende del m√©todo). |
| **Preservaci√≥n del significado original** | No preserva significado; pierde sem√°ntica del dominio. | S√≠ preserva el significado; √∫til para explicar decisiones de negocio. |
| **Cu√°ndo usarlo** | Cuando hay alta colinealidad, ruido o necesidad de compresi√≥n. | Cuando se requiere interpretabilidad o cuando algunas variables tienen se√±al clara. |
| **Resultado final** | Nuevos componentes sint√©ticos. | Subconjunto de variables originales y comprensibles. |

**Conclusi√≥n operativa:**  
PCA es ideal para *reducir ruido y colinealidad*, mientras que Feature Selection permite *mantener interpretabilidad* y concentrarse en las variables que aportan se√±al predictiva. En conjunto, ambos m√©todos permiten equilibrar eficiencia, claridad y precisi√≥n.

- - - 

## Insights clave  

- **Mutual Information** fue el m√©todo m√°s eficiente, combinando bajo error y alta velocidad.  
- **Wrapper Methods** (Forward/RFE) ofrecen gran precisi√≥n, pero son sensibles al tama√±o del dataset.  
- **PCA** es √∫til para reducir colinealidad, pero sacrifica interpretabilidad.  
- **Lasso** demostr√≥ capacidad para eliminar variables redundantes sin afectar la performance.  
- La combinaci√≥n **Filter + Wrapper** podr√≠a ofrecer el equilibrio ideal entre rendimiento y costo.

---

## Reflexi√≥n  

Esta pr√°ctica consolid√≥ el entendimiento de c√≥mo la **reducci√≥n de dimensionalidad** puede mejorar tanto la eficiencia como la generalizaci√≥n de los modelos de predicci√≥n.  
Mientras **PCA** act√∫a como un filtro matem√°tico que condensa informaci√≥n, los m√©todos de **Feature Selection** permiten conservar variables interpretables y √∫tiles desde el punto de vista del negocio.

El ejercicio demostr√≥ que la selecci√≥n de caracter√≠sticas no solo es una cuesti√≥n t√©cnica, sino tambi√©n estrat√©gica: elegir qu√© informaci√≥n conservar implica decidir **qu√© aspectos del problema son realmente relevantes**.  
Adem√°s, se evidenci√≥ que el mejor m√©todo depende del contexto: **los Filter Methods son ideales para datasets grandes y exploratorios**, mientras que los **Wrapper y Embedded Methods** son preferibles cuando se busca m√°xima precisi√≥n en modelos finales.

---

## Notebook en Google Colab  
üìì El notebook completo con el desarrollo de esta pr√°ctica puede consultarse en el siguiente enlace:  

---

## Referencias  
- [Scikit-learn: Feature Selection](https://scikit-learn.org/stable/modules/feature_selection.html)  
- [Principal Component Analysis ‚Äì Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)  
- [Feature Engineering & Selection ‚Äì Kuhn & Johnson, 2020](https://bookdown.org/max/FES/)  

---

## Navegaci√≥n  
‚¨ÖÔ∏è [Codificando la realidad: c√≥mo el encoding categ√≥rico mejora la predicci√≥n de ingresos en datos del censo](../practica9/main9.md)  
‚û°Ô∏è [Modelando el tiempo: c√≥mo el feature engineering temporal anticipa la recompra en e-commerce](../practica11/main11.md)  
üìì [√çndice del Portafolio](../../portfolio/index.md)
