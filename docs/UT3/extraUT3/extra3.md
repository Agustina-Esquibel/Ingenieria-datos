---
title: "Caso Retail Churn: c√≥mo el feature engineering anticipa el abandono de clientes"
date: 2025-11-20
---

## Contexto

Este caso adicional profundiza en t√©cnicas de **feature engineering, codificaci√≥n categ√≥rica, reducci√≥n dimensional y an√°lisis interpretativo** para comprender los factores que influyen en el abandono (*churn*) de clientes en un contexto de retail online.

El objetivo fue construir un pipeline reproducible capaz de transformar datos transaccionales en se√±ales predictivas, capturando patrones de comportamiento y enriqueciendo la representaci√≥n del cliente para modelos supervisados.  
El enfoque incluye construcci√≥n de features RFM, encoding de pa√≠ses, PCA para exploraci√≥n estructural e interpretaci√≥n del modelo mediante importancia de variables.

---

## Objetivos

- Dise√±ar features significativas basadas en comportamiento real del cliente.
- Implementar t√©cnicas de codificaci√≥n categ√≥rica avanzadas sin inflar dimensionalidad.
- Aplicar PCA para detectar estructuras internas y redundancias.
- Entrenar un modelo interpretable y analizar qu√© variables explican mejor el churn.
- Visualizar diferencias entre clientes retenidos y abandonados.
- Integrar un pipeline reproducible en Google Colab con limpieza, encoding y modelado.

---

## Desarrollo del caso

El trabajo se organiz√≥ en cuatro etapas:

### 1. Preparaci√≥n y construcci√≥n de features

Se dise√±aron variables clave basadas en la l√≥gica comercial del retail:

- **Recency**: d√≠as desde la √∫ltima compra.  
- **Frequency**: cantidad total de compras.  
- **Monetary value**: gasto acumulado por cliente.  
- **Engagement score**: m√©trica combinada que resume actividad reciente, diversidad de productos y recurrencia.

Estas variables capturan se√±ales tempranas de abandono y enriquecen la representaci√≥n del cliente.  
Se eligi√≥ este enfoque porque RFM y engagement son est√°ndares probados en an√°lisis de churn y permiten obtener interpretabilidad clara.

---

### 2. Codificaci√≥n categ√≥rica y tratamiento de pa√≠ses

Las variables de pa√≠s son tradicionalmente ruidosas y con alta cardinalidad relativa.  
Para evitar dimensionalidad excesiva se aplicaron **target encoding** y **one-hot encoding**, comparando impacto en el modelo y verificando su relaci√≥n con el churn.

El objetivo fue capturar patrones geogr√°ficos manteniendo un equilibrio entre representaci√≥n y estabilidad estad√≠stica.

---

### 3. Reducci√≥n dimensional con PCA

Para analizar la estructura interna de las features num√©ricas se aplic√≥ PCA.  
Esto permiti√≥:

- Identificar redundancias.  
- Observar cu√°nta varianza se conserva con pocos componentes.  
- Facilitar visualizaciones bidimensionales que representan grupos de clientes seg√∫n comportamiento.  

---

### 4. Entrenamiento del modelo y an√°lisis de importancia de features

Se entren√≥ un modelo de **gradient boosting**, dado su capacidad para capturar relaciones no lineales sin requerir excesiva ingenier√≠a adicional.

La importancia de features permiti√≥ identificar qu√© variables impactan m√°s en el abandono, aportando interpretabilidad directa para decisiones de negocio.

---

## Evidencias

### Distribuci√≥n de clientes por estado de churn  
![Distribuci√≥n de clientes por estado de churn](IMG_4701.png)

La distribuci√≥n revela un desbalance moderado: aproximadamente el doble de clientes retenidos respecto a abandonados.  
Este patr√≥n es habitual en contextos reales, donde el abandono es un evento menos frecuente.  
El desbalance implica que el modelo debe evaluarse cuidadosamente para evitar que la clase mayoritaria domine las predicciones.

---

### Recency (d√≠as desde √∫ltima compra) por estado de churn  
![Recency por churn](IMG_4702.png)

El boxplot muestra una diferencia marcada:

- Clientes **con churn** tienen recency muy superior (m√°s d√≠as sin comprar).  
- Clientes **retenidos** presentan recency mucho menor y m√°s concentrado.

Este resultado confirma la relevancia de *recency* como predictor clave, validando la elecci√≥n del feature RFM.

---

### Importancia de features en la predicci√≥n de churn  
![Importancia de features](IMG_4703.png)

Las variables m√°s influyentes fueron:

- **Recency**: principal se√±al de abandono.  
- **Engagement score**: resume comportamiento y actividad reciente.  
- **Tenure days**: antig√ºedad del cliente.  
- **Log-monetary y frecuencia**: indicadores de valor y recurrencia.

Los pa√≠ses aparecen con baja importancia, lo que confirma que la codificaci√≥n categ√≥rica no domina indebidamente al modelo.

---

### PCA ‚Äì Varianza explicada acumulada  
![PCA varianza](IMG_4704.png)

El PCA revela que:

- Los **primeros dos componentes** explican m√°s del 70% de la varianza.  
- Con **cuatro componentes** se supera el 85%.  

Esto indica que las features num√©ricas tienen estructura clara y redundancias que pueden comprimirse sin perder demasiada informaci√≥n.

---

### Proyecci√≥n de clientes en PC1‚ÄìPC2 coloreada por churn  
![Clientes proyectados](IMG_4705.png)

El gr√°fico muestra que:

- Los clientes abandonados (rojo) tienden a agruparse en zonas asociadas a **recency alta y engagement bajo**.
- Los clientes retenidos (azul) est√°n m√°s dispersos en √°reas de menor riesgo.  
- Aunque no existe una separaci√≥n lineal perfecta, s√≠ se observan patrones comportamentales que justifican el modelado.

Esta visualizaci√≥n ayuda a interpretar c√≥mo se distribuyen los clientes en un espacio reducido y valida la utilidad del PCA para exploraci√≥n.

---

## Insights clave

- **Recency y engagement** son los predictores dominantes del churn.  
- La ingenier√≠a de features basada en comportamiento supera ampliamente a las variables originales del dataset.  
- La codificaci√≥n categ√≥rica controlada evita dimensionalidad excesiva sin perder informaci√≥n relevante.  
- El PCA muestra que la estructura del comportamiento del cliente puede representarse con pocos componentes.  
- La combinaci√≥n de RFM + engagement + modelado interpretativo produce un pipeline s√≥lido y replicable.

---

## Reflexi√≥n

El caso demuestra que el **feature engineering es el n√∫cleo del an√°lisis de churn**.  
Las transformaciones aplicadas permitieron capturar se√±ales tempranas de abandono, facilitando tanto la interpretaci√≥n como la capacidad predictiva del modelo.

Adem√°s, la combinaci√≥n de t√©cnicas ‚ÄîRFM, encoding, PCA y modelos interpretables‚Äî evidencia c√≥mo cada decisi√≥n t√©cnica influye en la calidad final del an√°lisis y en su aplicabilidad real dentro del negocio.

Este ejercicio refuerza la importancia de:

- dise√±ar features alineadas al dominio,  
- evaluar estructuras internas antes de modelar,  
- y usar visualizaciones para interpretar el comportamiento del cliente.  

---

## Notebook en Google Colab

üììEl notebook completo con el desarrollo de est√° pr√°ctica puede consultarse en el siguiente enlace:

üîó[Abrir notebook en Google Colab](https://colab.research.google.com/github/Agustina-Esquibel/Ingenieria-datos/blob/main/docs/UT3/extraUT3/UT3extra.ipynb)

---

## Referencias

- Feature Engineering for Machine Learning ‚Äì Zheng & Casari, O‚ÄôReilly.  
- Scikit-learn preprocessing & decomposition documentation.  
- Customer Analytics frameworks: RFM & engagement modeling.  
- Kaggle Feature Engineering Course.

---

## Navegaci√≥n

‚¨ÖÔ∏è [Volver a Unidad Tem√°tica 3](../main.md)  
üìì [√çndice del portafolio](../../portfolio/index.md)
