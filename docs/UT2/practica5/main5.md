---
title: Ames Housing bajo la lupa ‚Äî Reconstrucci√≥n de informaci√≥n faltante con imputaci√≥n contextual
date: 2025-09-10
---

# Contexto y Alcance

El conjunto de datos **Ames Housing (Iowa)** contiene informaci√≥n detallada sobre propiedades inmobiliarias: dimensiones, materiales, ubicaci√≥n, estado y precios de venta. Su amplitud, heterogeneidad y presencia de valores faltantes lo convierten en un caso ideal para estudiar calidad de datos y evaluar el impacto del *missing data* en procesos predictivos.

Esta pr√°ctica aborda un flujo completo orientado a **identificar, entender y reconstruir informaci√≥n faltante**, utilizando un enfoque contextual que respeta el significado real de cada ausencia.  
El an√°lisis combina **imputaci√≥n simple** y **una imputaci√≥n inteligente basada en reglas estructurales**, permitiendo comparar ambos m√©todos y justificar decisiones t√©cnicas y √©ticas.

---

# Objetivos

- Clasificar los valores faltantes seg√∫n su mecanismo (MCAR, MAR o MNAR).  
- Evaluar el impacto del *missing data* en la integridad del dataset.  
- Comparar dos enfoques de imputaci√≥n aplicables al dataset.  
- Implementar un pipeline reproducible con *scikit-learn*.  
- Documentar criterios de decisi√≥n y reflexiones √©ticas del proceso.  

---

# Desarrollo

El an√°lisis se realiz√≥ en **Google Colab** empleando `pandas`, `numpy`, `matplotlib`, `seaborn`, `missingno` y `scikit-learn`.  
El flujo metodol√≥gico sigui√≥ las etapas del modelo **CRISP-DM**, con foco en calidad y √©tica de datos.

---

## 1. Comprensi√≥n del problema

El an√°lisis inicial indic√≥ que los valores faltantes **no se distribu√≠an aleatoriamente**.  
Se observaron patrones coherentes con la infraestructura de la vivienda (ej.: ausencia de s√≥tano ‚Üí valores nulos en variables de s√≥tano).  
Esto permiti√≥ descartar el mecanismo **MCAR** y orientar el diagn√≥stico hacia **MAR** y **MNAR**.

---

## 2. Comprensi√≥n de los datos

Heatmaps, correlaciones y an√°lisis exploratorio mostraron que:

- Las variables opcionales (garage, piscina, s√≥tano) ten√≠an valores faltantes **sistem√°ticos**.  
- Las columnas con >90 % de nulos correspond√≠an a caracter√≠sticas raras (ej.: calidad de piscina).  
- Varias ausencias proven√≠an de una **condici√≥n estructural** (ausencia real del feature), no de errores de registro.

Esto justific√≥ usar **imputaci√≥n contextual** en lugar de m√©todos globales indiferenciados.

---

## 3. Estrategias de imputaci√≥n: dos enfoques comparados

### **Enfoque 1 ‚Äî Imputaci√≥n simple (baseline)**  
Implementada con `SimpleImputer`:

- Num√©ricas ‚Üí **mediana**  
- Categ√≥ricas ‚Üí **moda**  

**Criterio de decisi√≥n:**  
La imputaci√≥n simple sirve como *baseline* reproducible, r√°pida y √∫til cuando la distribuci√≥n no est√° sesgada. Permite evaluar si es suficiente para mantener la estructura original del dataset.

---

### **Enfoque 2 ‚Äî Imputaci√≥n inteligente basada en reglas contextuales**

Implementada en el notebook mediante la funci√≥n `smart_imputation()` del Paso 10.  
Incluye:

- Reemplazo de nulos por categor√≠as reales como **"NoGarage"**, **"NoBasement"**, etc.  
- Preservaci√≥n del significado estructural del dato.  
- Evita que la ausencia real sea confundida con la moda, que introduce un sesgo artificial.

**Criterio de decisi√≥n:**  
Este m√©todo se ajusta mejor al contexto del dataset: cuando la ausencia representa una caracter√≠stica f√≠sica, imputar la moda genera **falsas categor√≠as** y distorsiona relaciones reales.

---

## 4. Comparaci√≥n entre los dos m√©todos

Para evaluar impacto de la imputaci√≥n, se compararon:

- Cambio absoluto de correlaciones  
- Variaci√≥n de medianas en variables num√©ricas  
- Cantidad de filas donde la imputaci√≥n altera relaciones estructurales  

**Tabla comparativa (m√©trica resumida):**

| M√©trica evaluada | Imputaci√≥n Simple | Imputaci√≥n Inteligente | Conclusi√≥n |
|------------------|------------------:|------------------------:|-----------|
| Cambio medio en correlaciones | 0.074 | **0.031** | Inteligente preserva mejor la estructura |
| Categor√≠as artificiales creadas | **Alta** | **Ninguna** | La simple genera ruido sem√°ntico |
| Sesgo introducido en variables opcionales | Medio | **Bajo** | Enfoque contextual evita distorsiones |
| Interpretabilidad | Baja | **Alta** | Mantiene significado real de la ausencia |

**Conclusi√≥n operativa:**  
‚û° La **imputaci√≥n inteligente** reduce sesgos, preserva relaciones estructurales y evita categor√≠as err√≥neas.  
‚û° La **imputaci√≥n simple** funciona como baseline, pero no es adecuada para variables estructurales.

---

## 5. Evaluaci√≥n del proceso

Histogramas, boxplots y comparaci√≥n de distribuciones mostraron que:

- La imputaci√≥n inteligente mantiene la forma de las distribuciones originales.  
- El m√©todo simple aumenta la frecuencia de valores comunes, afectando varianza.  
- Ning√∫n m√©todo distorsion√≥ severamente la estructura global, pero el enfoque inteligente mostr√≥ **mejor estabilidad**.

---

## 6. Documentaci√≥n √©tica

Cada decisi√≥n fue tomada considerando:

- Evitar introducir informaci√≥n falsa.  
- No reemplazar ausencias estructurales con categor√≠as que la vivienda **nunca tuvo**.  
- Mantener trazabilidad completa del proceso (antes y despu√©s).  

Este enfoque promueve transparencia, reproducibilidad y responsabilidad profesional.

---

# Evidencias

A continuaci√≥n se presenta el conjunto de visualizaciones utilizadas para fundamentar decisiones metodol√≥gicas.


### 1. Porcentaje de Missing por columna  
**Conclusi√≥n:** Varias columnas superan el 80‚Äì90 % de ausencias, indicando **MNAR estructural** y justificando su eliminaci√≥n o imputaci√≥n contextual.

---

### 2. Distribuci√≥n de Missing por fila  
**Conclusi√≥n:** La mayor√≠a de registros tiene entre 4 y 7 valores ausentes, lo que permite usar imputaci√≥n sin eliminar filas completas.

---

### 3. Mapa de calor de correlaci√≥n antes y despu√©s de la imputaci√≥n  
**Conclusi√≥n:** El Enfoque 2 preserva mejor la estructura correlacional, con menor variaci√≥n en relaciones clave.

---

### 4. Comparaci√≥n visual de la imputaci√≥n en variables categ√≥ricas  
(Ej.: `GarageType`, `BsmtExposure`, `PoolQC`)  
**Conclusi√≥n:** La imputaci√≥n simple introduce categor√≠as artificiales; la inteligente asigna valores sem√°nticamente correctos como ‚ÄúNoGarage‚Äù.

---

### 5. Boxplots comparando distribuciones originales vs imputadas (num√©ricas)  
**Conclusi√≥n:** La imputaci√≥n por mediana altera m√≠nimamente las distribuciones; la imputaci√≥n inteligente mantiene mejor la forma para variables opcionales.

---

### 6. Heatmap de valores imputados por m√©todo  
**Conclusi√≥n:** La imputaci√≥n simple rellena de manera uniforme; la inteligente muestra un patr√≥n contextual alineado con la estructura del inmueble.

---

# Insights clave

1. El 15 % de las variables presentaban m√°s del 20 % de faltantes.  
2. La mayor√≠a de ausencias respond√≠a a patrones **MAR** y **MNAR** estructurales.  
3. La imputaci√≥n inteligente evit√≥ sesgos y categor√≠as artificiales.  
4. La comparaci√≥n de m√©todos permiti√≥ justificar decisiones de limpieza.  
5. La calidad del dataset depende tanto de la completitud como del entendimiento del **contexto**.

---

# Reflexi√≥n

La reconstrucci√≥n de datos faltantes no es un ejercicio mec√°nico: exige comprender el origen de las ausencias, evaluar riesgos de sesgo y tomar decisiones informadas que preserven la veracidad del dataset.  
Comparar dos enfoques revel√≥ que los m√©todos globales pueden introducir ruido sem√°ntico, mientras que la imputaci√≥n contextual respeta la estructura de la informaci√≥n y mejora la interpretabilidad.

Este proceso fortaleci√≥ habilidades metodol√≥gicas, √©ticas y t√©cnicas, reafirmando la importancia de la **transparencia** y la **reproducibilidad** en la ingenier√≠a de datos.

---

# Notebook en Google Colab

üìì El notebook completo con el desarrollo de esta pr√°ctica puede consultarse en el siguiente enlace:

üîó https://colab.research.google.com/github/Agustina-Esquibel/ingenieria-datos/blob/main/docs/UT2/practica5/UT2_practica5.ipynb

---

# Referencias

- Ames Housing Dataset, Kaggle Repository.  
- Kaggle. *Data Cleaning Course.*  
- Pandas Documentation ‚Äî *Handling Missing Data.*  
- Scikit-learn Documentation ‚Äî *Imputation and Pipeline Construction.*

---

# Navegaci√≥n

- [‚¨ÖÔ∏è Volver a Unidad Tem√°tica 2](../main.md)  
- [‚û°Ô∏è Ir a Pr√°ctica 6 ‚Äî Feature Scaling y Anti-Leakage Pipeline](../practica6/main6.md)  
- [üìì √çndice del Portafolio](../../portfolio/index.md)
