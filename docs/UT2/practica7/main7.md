---
title: Sesgo bajo la lupa ‚Äî Detecci√≥n, mitigaci√≥n y decisiones √©ticas con Fairlearn
date: 2025-09-24
---

# Contexto y Alcance

Los modelos de aprendizaje autom√°tico reproducen patrones presentes en los datos hist√≥ricos que los entrenan.  
Cuando esos datos contienen desigualdades estructurales ‚Äîraciales, socioecon√≥micas, de g√©nero‚Äî el modelo puede amplificarlas, afectando decisiones sensibles.  

Esta pr√°ctica aborda **la detecci√≥n y mitigaci√≥n de sesgos algor√≠tmicos** mediante dos casos complementarios:

1. **Boston Housing (Regresi√≥n):** dataset que incluye una variable racial controversial que influye en el precio de la vivienda.  
2. **Titanic (Clasificaci√≥n):** dataset donde aparecen diferencias marcadas en tasas de supervivencia seg√∫n g√©nero y clase social.

El trabajo se centra en **medir sesgos, aplicar m√©todos de mitigaci√≥n con Fairlearn y analizar las implicancias √©ticas** de cada decisi√≥n.

---

# Objetivos

- Detectar y cuantificar desigualdades presentes en modelos predictivos.  
- Analizar el rol de las variables sensibles y su impacto en la equidad.  
- Aplicar t√©cnicas de fairness (`ExponentiatedGradient`, *constraints* de Paridad Demogr√°fica).  
- Evaluar el compromiso entre precisi√≥n y justicia.  
- Reflexionar sobre el rol √©tico del cient√≠fico de datos frente a sesgos estructurales.

---

# Desarrollo

## 1. Carga, reconstrucci√≥n y preparaci√≥n de los datasets

- **Boston Housing:**  
  - Se reconstruy√≥ la variable racial original (`B`) mediante la f√≥rmula hist√≥rica.  
  - Se explor√≥ la relaci√≥n entre composici√≥n racial y valor de la vivienda.  

- **Titanic:**  
  - Se imputaron edades faltantes y se codificaron variables categ√≥ricas.  
  - Se identificaron las variables sensibles: **g√©nero** y **clase**.

## 2. An√°lisis inicial del sesgo

- **Boston:** se midi√≥ la correlaci√≥n entre la variable racial y el precio medio de la vivienda (MEDV).  
  Esta correlaci√≥n negativa refleja desigualdades hist√≥ricas en distribuci√≥n territorial y pol√≠tica de cr√©dito.

- **Titanic:** se calcularon tasas de supervivencia por g√©nero y clase.  
  Las mujeres presentan el doble de probabilidad de sobrevivir respecto a los hombres; la 1¬™ clase supera ampliamente al resto.

**Criterio de decisi√≥n:**  
El an√°lisis exploratorio permite determinar **d√≥nde** y **c√≥mo** se manifiesta el sesgo, para seleccionar el m√©todo de mitigaci√≥n adecuado.

## 3. Entrenamiento de modelos baseline

- **Boston Housing:** se entrenaron dos modelos de regresi√≥n lineal:  
  - Modelo A: incluye variable racial.  
  - Modelo B: excluye variable racial.  

  Esto permite medir cu√°nto aporta la variable sensible al performance y al sesgo.

- **Titanic:** se entren√≥ un **Random Forest baseline** sin restricciones de equidad.

**Aprendizaje:**  
Los modelos sin control de fairness pueden exhibir buen rendimiento num√©rico, aun siendo profundamente injustos en subgrupos sensibles.

## 4. Mitigaci√≥n con Fairlearn

Se aplic√≥ el m√©todo **ExponentiatedGradient** con el constraint **Demographic Parity** usando g√©nero como variable sensible.

**Criterio de decisi√≥n:**  
La paridad demogr√°fica permite evaluar si el modelo asigna predicciones positivas con la misma frecuencia en distintos grupos, independientemente del resultado real.  
Se eligi√≥ este m√©todo porque facilita comparar el ‚Äúantes vs despu√©s‚Äù sin modificar el dataset original.

## 5. Evaluaci√≥n final

Se analizaron dos dimensiones:

1. **Rendimiento (accuracy / MAE).**  
2. **Equidad (disparidad entre grupos).**

Esto permiti√≥ estudiar el cl√°sico *trade-off* entre precisi√≥n y justicia.

---

# Evidencias

## üìä Disparidad entre g√©neros ‚Äì Antes vs Despu√©s de Fairlearn
![Disparidad entre g√©neros ‚Äì Antes vs Despu√©s de Fairlearn](Fairlearn.png)

**Observaci√≥n t√©cnica:**  
La disparidad demogr√°fica cae de ~0.11 en el modelo baseline a ~0.06 tras aplicar Fairlearn.

**Interpretaci√≥n anal√≠tica:**  
El modelo corregido disminuye la brecha entre hombres y mujeres en la probabilidad de supervivencia predicha.

**Conclusi√≥n operativa:**  
La mitigaci√≥n algor√≠tmica logra mejorar la equidad **sin tocar los datos**, actuando directamente sobre el proceso de entrenamiento.

---

## üìä Tasa predicha de supervivencia por g√©nero
![Tasa predicha de supervivencia por g√©nero](Tasa_predicha.png)

**Observaci√≥n t√©cnica:**  
El modelo baseline favorece sistem√°ticamente a las mujeres; Fairlearn reduce esa diferencia y acerca las tasas.

**Interpretaci√≥n anal√≠tica:**  
El modelo mitigado deja de reproducir el patr√≥n ‚Äúmujeres sobreviven m√°s‚Äù, evitando una repetici√≥n acr√≠tica de sesgos sociales.

**Conclusi√≥n operativa:**  
El ajuste suaviza diferencias injustificadas, manteniendo la estructura general de los datos sin eliminar informaci√≥n relevante.

---

## üìä Trade-off entre rendimiento y equidad
![Trade-off entre rendimiento y equidad](Rendimiento_equidad.png)

**Observaci√≥n t√©cnica:**  
El accuracy baja de ~68% a ~64%, pero la disparidad se reduce casi a la mitad.

**Interpretaci√≥n anal√≠tica:**  
La equidad exige sacrificar una porci√≥n del rendimiento; este descenso es natural y esperado.

**Conclusi√≥n operativa:**  
Un modelo ‚Äúmenos preciso pero m√°s justo‚Äù suele ser preferible en contextos sociales o sensibles.

---

## üìä Distribuci√≥n de precios por grupo racial (Boston Housing)
![Distribuci√≥n de precios por grupo racial](Boxplot.png)

**Observaci√≥n t√©cnica:**  
Las zonas con mayor proporci√≥n de poblaci√≥n afroamericana muestran valores medios m√°s bajos y mayor dispersi√≥n.

**Interpretaci√≥n anal√≠tica:**  
El modelo puede aprender asociaciones hist√≥ricas injustas entre raza y precio, transfiriendo desigualdades al proceso predictivo.

**Conclusi√≥n operativa:**  
La variable racial debe usarse con extrema cautela y, en la mayor√≠a de los casos, excluirse del modelo productivo.

---

# Insights clave

1. Los modelos pueden ser precisos pero profundamente injustos.  
2. Las variables sensibles no deben eliminarse sin analizar primero su impacto.  
3. Los m√©todos de fairness permiten cuantificar desigualdades de forma estructurada.  
4. El *trade-off* entre rendimiento y equidad es inevitable y debe gestionarse deliberadamente.  
5. La evaluaci√≥n √©tica es parte integral del ciclo de vida del modelo.

---

# Reflexi√≥n

El experimento mostr√≥ que **no existe neutralidad algor√≠tmica**.  
Los modelos aprenden del mundo tal como es, no como deber√≠a ser, y pueden amplificar desigualdades invisibles si no se interviene.

En **Boston Housing**, la variable racial mejora el ajuste, pero perpet√∫a diferencias hist√≥ricas en valor de vivienda.  
La decisi√≥n de excluirla representa un acto de responsabilidad profesional.

En **Titanic**, las diferencias de supervivencia son reales, pero un modelo destinado a decisiones modernas debe evitar replicarlas acr√≠ticamente.  
La mitigaci√≥n con Fairlearn permiti√≥ construir un modelo m√°s equilibrado, revelando que la equidad es alcanzable sin sacrificar completamente el rendimiento.

La pr√°ctica deja una lecci√≥n central:  
**la √©tica no es un complemento t√©cnico, sino un requisito para construir modelos v√°lidos, confiables y justos**.

---

# Notebook en Google Colab

üìì El notebook completo con el desarrollo de esta pr√°ctica puede consultarse en el siguiente enlace:

üîó [Abrir en Google Colab](https://colab.research.google.com/github/Agustina-Esquibel/Ingenieria-datos/blob/main/docs/UT2/practica7/UT2_practica7.ipynb)

---

# Referencias

- Fairlearn Documentation  
- Scikit-learn: *Fairness, Ethics & Model Evaluation*  
- CMU Boston Housing Dataset (1978)  
- Titanic Dataset (Kaggle / Seaborn)

---

# Navegaci√≥n

[‚¨ÖÔ∏è Volver a Pr√°ctica 6 ‚Äî Feature Scaling y Anti-Leakage Pipeline](../practica6/main6.md)  
[‚¨ÖÔ∏è Volver a UT2](../main.md)  
[üìì √çndice del Portafolio](../../portfolio/index.md)
