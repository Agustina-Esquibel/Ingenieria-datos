---
title: Sesgo bajo la lupa ‚Äî Detecci√≥n, correcci√≥n y decisiones √©ticas con Fairlearn
date: 2025-09-24
---

## Contexto

Los modelos de aprendizaje autom√°tico aprenden patrones a partir de datos hist√≥ricos, y cuando esos datos reflejan desigualdades estructurales, el modelo puede amplificarlas.  
La detecci√≥n y correcci√≥n de sesgos se vuelve entonces una tarea esencial para garantizar el uso responsable de la inteligencia artificial.

En esta pr√°ctica se trabaj√≥ con dos casos de estudio complementarios:  

1. **Boston Housing (Regresi√≥n):** dataset eliminado de scikit-learn por contener una variable racial problem√°tica que codifica la proporci√≥n de poblaci√≥n afroamericana.  
2. **Titanic (Clasificaci√≥n):** dataset donde las diferencias de supervivencia reproducen sesgos sociales basados en g√©nero y clase social.

El objetivo fue aplicar m√©todos de fairness mediante la librer√≠a **Fairlearn**, para detectar, medir y mitigar sesgos, evaluando tanto el impacto t√©cnico como las implicancias √©ticas.

---

## Objetivos

- Detectar la presencia de sesgos en modelos predictivos basados en datos reales.  
- Analizar c√≥mo las variables sensibles afectan la equidad del modelo.  
- Implementar estrategias de correcci√≥n de sesgo y evaluar su impacto en rendimiento.  
- Comprender el equilibrio entre precisi√≥n, equidad y responsabilidad profesional.  
- Reflexionar sobre las decisiones √©ticas involucradas en el desarrollo de modelos.

---

## Desarrollo

1. **Carga y exploraci√≥n de datasets.**  
   Se cargaron los conjuntos de datos *Boston Housing* y *Titanic*.  
   En el caso de Boston, se reconstruy√≥ la variable racial `Bk_racial` a partir de la f√≥rmula original eliminada de la versi√≥n actual de scikit-learn.  
   En Titanic, se prepararon las variables num√©ricas y se eliminaron valores faltantes en edad y puerto de embarque.

2. **An√°lisis del sesgo.**  
   En Boston se calcul√≥ la correlaci√≥n entre la variable racial y el precio medio de las viviendas (`MEDV`).  
   En Titanic se calcularon las tasas de supervivencia por g√©nero y clase, confirmando una diferencia significativa a favor de las mujeres y los pasajeros de primera clase.

3. **Entrenamiento de modelos base.**  
   - Para Boston, se utiliz√≥ **regresi√≥n lineal** incluyendo y excluyendo la variable racial.  
   - Para Titanic, se aplic√≥ un **Random Forest** sin restricciones de equidad como modelo baseline.  

4. **Aplicaci√≥n de Fairlearn.**  
   En el caso Titanic, se utiliz√≥ el m√©todo **ExponentiatedGradient** con la restricci√≥n de *Demographic Parity*.  
   Se entren√≥ un modelo ajustado que busc√≥ equilibrar las tasas de predicci√≥n positiva entre hombres y mujeres.

5. **Evaluaci√≥n y comparaci√≥n.**  
   Se compararon los resultados de precisi√≥n y de equidad antes y despu√©s de aplicar la correcci√≥n, cuantificando la p√©rdida de rendimiento y la ganancia en justicia algor√≠tmica.

---

## Evidencias

### Brecha racial en Boston Housing

La visualizaci√≥n muestra la diferencia promedio de precios entre zonas con alta y baja proporci√≥n afroamericana.  
Los resultados indican una **brecha de aproximadamente 8.500 d√≥lares**, equivalente al **18 % del valor medio de las viviendas**.  
Esto evidencia un sesgo estructural hist√≥rico en el dataset, que se traslada directamente al modelo de regresi√≥n si la variable racial es incluida.

---

### Distribuci√≥n de precios por grupo racial

El histograma y boxplot comparan las distribuciones de precios de los dos grupos.  
Se observa que las zonas con mayor proporci√≥n afroamericana presentan precios m√°s bajos y menor variabilidad, mientras que las dem√°s zonas exhiben precios medios y dispersiones superiores.  
Esta desigualdad se traduce en predicciones sistem√°ticamente m√°s bajas para ciertos grupos poblacionales.

---

### Sesgo de g√©nero en el modelo Titanic (baseline)

El gr√°fico muestra la tasa de supervivencia predicha por g√©nero en el modelo baseline.  
Las mujeres presentan una probabilidad de supervivencia un **33 % mayor** que los hombres.  
Aunque este resultado refleja la realidad del hecho hist√≥rico, en t√©rminos algor√≠tmicos representa un **sesgo de clasificaci√≥n injusto** que favorece a un grupo.

---

### Modelo corregido con Fairlearn

Tras aplicar el m√©todo **ExponentiatedGradient**, la disparidad entre g√©neros se reduce considerablemente.  
El *Demographic Parity Difference* pasa de **0.27 a 0.10**, mientras que la p√©rdida de rendimiento (accuracy) fue m√≠nima, pasando de **0.79 a 0.76**.  
La visualizaci√≥n comparativa confirma una distribuci√≥n m√°s balanceada de predicciones entre hombres y mujeres.

---

### Trade-off entre rendimiento y equidad

El gr√°fico de trade-off muestra el equilibrio logrado entre precisi√≥n y justicia.  
El modelo justo perdi√≥ un **3,8 % de rendimiento**, pero mejor√≥ la equidad en **0.17 puntos**, logrando un compromiso √≥ptimo.  
El resultado respalda la idea de que es posible **reducir sesgos sin comprometer en exceso la capacidad predictiva**.

---

## Insights clave

1. Los modelos precisos pueden ser injustos si no se analizan m√©tricas de equidad.  
2. Las variables sensibles no deben eliminarse sin antes medir su efecto en el sesgo.  
3. El contexto define la tolerancia √©tica al sesgo: en √°mbitos sociales, la equidad es prioritaria.  
4. Fairlearn permite cuantificar y mitigar desigualdades de manera controlada y reproducible.  
5. Rendimiento y equidad no son opuestos: con un dise√±o √©tico, pueden coexistir.

---

## Reflexi√≥n

El caso **Boston Housing** muestra c√≥mo el uso de variables sensibles puede introducir discriminaciones hist√≥ricas en un modelo.  
Aunque la variable `B` mejora el ajuste, representa informaci√≥n que perpet√∫a desigualdades.  
Excluirla de los modelos de producci√≥n es una decisi√≥n √©tica necesaria, aun cuando implique una leve p√©rdida de rendimiento.

El caso **Titanic** evidencia c√≥mo los sesgos de contexto (g√©nero, clase) pueden afectar la imparcialidad de los resultados.  
La correcci√≥n mediante *Fairlearn* demostr√≥ que es posible alcanzar un modelo m√°s equilibrado, validando la importancia de incluir m√©tricas de equidad en el proceso de modelado.

Ambos ejercicios llevan a una conclusi√≥n central: **no existe neutralidad algor√≠tmica**.  
Los modelos reproducen los valores y estructuras de quienes los construyen.  
Por ello, la √©tica no es un complemento del proceso t√©cnico, sino un requisito esencial para garantizar la validez y la justicia de los sistemas basados en datos.

---

## Notebook en Google Colab

üìì El notebook completo con el desarrollo de esta pr√°ctica puede consultarse en el siguiente enlace:

üîó [Abrir en Google Colab](https://colab.research.google.com/github/Agustina-Esquibel/Ingenieria-datos/blob/main/docs/UT2/practica7/UT2_practica7.ipynb)


---

## Referencias

- Fairlearn Documentation (Microsoft Research).  
- Scikit-learn: *Model Evaluation and Ethics in Machine Learning*.  
- CMU Boston Housing Dataset (1978).  
- Titanic Dataset (Kaggle / Seaborn).  

---

## Navegaci√≥n
