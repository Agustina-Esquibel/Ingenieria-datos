---
title: "Caso German Credit: cÃ³mo la calidad del dato condiciona la equidad en modelos financieros"
date: 2025-11-17
---

## Contexto

Esta prÃ¡ctica adicional se centra en el anÃ¡lisis de **calidad del dato**, **preprocesamiento responsable** y **evaluaciÃ³n Ã©tica** aplicada a un caso realista de riesgo crediticio.  
El objetivo es comprender cÃ³mo decisiones tÃ©cnicas â€”imputaciÃ³n, codificaciÃ³n, escalado y validaciÃ³nâ€” impactan en la equidad del modelo, especialmente en variables sensibles como el gÃ©nero.

Se trabajÃ³ con el dataset **German Credit**, ampliamente utilizado para estudiar escenarios de riesgo, sesgos estructurales y fairness en sistemas financieros.  
El foco estuvo puesto en detectar problemas de representatividad, analizar correlaciones, evitar data leakage, evaluar sesgo en las predicciones y reflexionar sobre el impacto social de los modelos.

---

## Objetivos

- Identificar problemas de calidad del dato y aplicar imputaciÃ³n reproducible.  
- Detectar patrones de desbalance y su impacto en modelos supervisados.  
- Prevenir *data leakage* mediante anÃ¡lisis estructural del dataset.  
- Implementar un pipeline reproducible de limpieza, encoding y escalado.  
- Evaluar **fairness** utilizando mÃ©tricas segmentadas por gÃ©nero.  
- Reflexionar sobre implicancias Ã©ticas en la toma de decisiones automatizadas.

---

## Desarrollo

El desarrollo siguiÃ³ un enfoque incremental y reproducible en Google Colab.

Primero, se cargÃ³ el dataset German Credit y se realizÃ³ una inspecciÃ³n inicial para detectar valores faltantes, inconsistencias y desbalance entre clases. Se aplicaron tÃ©cnicas de imputaciÃ³n adecuadas segÃºn el tipo de variable, asegurando coherencia con el dominio financiero.

Luego, se construyÃ³ un pipeline que incluyÃ³:

- imputaciÃ³n numÃ©rica y categÃ³rica,  
- codificaciÃ³n OneHot de variables nominales,  
- escalado estandarizado,  
- validaciÃ³n estratificada para preservar la proporciÃ³n de clases.  

Posteriormente, se entrenÃ³ un modelo base para analizar rendimiento y fairness, comparando mÃ©tricas entre subgrupos sensibles (gÃ©nero). Se generaron visualizaciones especÃ­ficas para evidenciar la distribuciÃ³n de clases, las correlaciones entre atributos numÃ©ricos y el comportamiento de las predicciones del modelo.

Finalmente, se evaluÃ³ el impacto Ã©tico del sistema, considerando cÃ³mo la estructura del dataset y las decisiones de preprocesamiento afectan la transparencia, equidad y confiabilidad del modelo.

---

## Evidencias

## DistribuciÃ³n de clases en el dataset  
![DistribuciÃ³n de clases](IMG_4690.png)

La distribuciÃ³n evidencia un **desbalance moderado**, donde la clase *good credit* domina el dataset.  
Esto es relevante porque los modelos tienden a **favorecer la clase mayoritaria**, aumentando la probabilidad de errores sistemÃ¡ticos sobre la clase minoritaria (*bad credit*).  

Este tipo de anÃ¡lisis es clave en UT2 porque permite entender el impacto del **balance de clases en sesgos posteriores**.

---

## Matriz de correlaciÃ³n numÃ©rica (tras simulaciÃ³n de missing)  
![Matriz de correlaciÃ³n](IMG_4689.png)

La matriz muestra relaciones importantes:

- `credit_amount` y `duration` presentan una **correlaciÃ³n fuerte y positiva** (montos mayores â†’ plazos mÃ¡s largos).  
- Variables como `age` o `existing_credits` no muestran correlaciones relevantes, lo que sugiere baja multicolinealidad.  
- La simulaciÃ³n de *missing data* mantuvo la estructura general, permitiendo evaluar cÃ³mo responderÃ­an los mÃ©todos de imputaciÃ³n.

Este anÃ¡lisis es central para UT2, ya que permite fundamentar decisiones de **imputaciÃ³n contextual** y evitar interpretaciones sesgadas.

---

## ProporciÃ³n de â€œbuen crÃ©ditoâ€ predicho por gÃ©nero  
![ProporciÃ³n por gÃ©nero](IMG_4691.png)

El modelo predice una **proporciÃ³n ligeramente mayor de â€œgood creditâ€ para hombres**, indicador de un posible sesgo en la clasificaciÃ³n.  

Aunque el dataset contiene variables limitadas de gÃ©nero, este resultado demuestra cÃ³mo **la estructura de los datos puede inducir desigualdad**, incluso en modelos simples.

Es un punto clave de la UT2 en relaciÃ³n a **fairness y mÃ©tricas de equidad**.

---

## Curva ROC segmentada por gÃ©nero  
![Curva ROC](IMG_4692.png)

El anÃ¡lisis ROC muestra:

- Un **AUC â‰ˆ 0.78 para hombres**, lo que indica un rendimiento aceptable del modelo.  
- La ausencia de curvas para mujeres revela un problema de **representatividad**, ya que el dataset ofrece informaciÃ³n insuficiente sobre otros grupos.

Esto demuestra una idea crÃ­tica de la UT2:  
> *No se puede evaluar fairness si los datos ya estÃ¡n sesgados en origen.*

---

## Insights clave

- La calidad del dato tiene un impacto directo en la equidad del modelo: problemas de representatividad conducen a predicciones injustas.  
- La evaluaciÃ³n de fairness no consiste sÃ³lo en medir rendimiento global, sino en analizar subgrupos sensibles.  
- El desbalance de clases afecta no solo al rendimiento, sino tambiÃ©n a la estabilidad Ã©tica del sistema.  
- La matriz de correlaciÃ³n permite descartar riesgos de *leakage* y comprender relaciones relevantes antes del modelado.  
- La ingenierÃ­a de preprocesamiento (encoding, imputaciÃ³n, escalado) es tan importante como el algoritmo utilizado.

---

## ReflexiÃ³n

Esta prÃ¡ctica permitiÃ³ comprender que **la Ã©tica y la calidad del dato estÃ¡n profundamente interconectadas**.  
Un modelo financiero puede presentar mÃ©tricas sÃ³lidas en general, pero aun asÃ­ producir desigualdad si los datos no representan de forma equitativa a todos los usuarios.

El caso German Credit ilustra cÃ³mo decisiones tÃ©cnicas aparentemente inocuas â€”como imputaciÃ³n, encoding o selecciÃ³n de variablesâ€” pueden derivar en impactos sociales significativos.  
La ausencia de predicciones para un grupo sensible evidencia que la equidad no surge de forma automÃ¡tica: debe ser diseÃ±ada, monitoreada y evaluada expresamente.

En sÃ­ntesis, esta prÃ¡ctica refuerza la visiÃ³n central de UT2:  
**la responsabilidad en ciencia de datos implica asegurar calidad, transparencia y justicia en cada etapa del pipeline.**

---

## Notebook en Google Colab  

ğŸ““El notebook completo con el desarrollo de esta prÃ¡ctica puede consultarse en el siguiente enlace:

ğŸ”—[**Abrir notebook en Google Colab**](https://colab.research.google.com/github/Agustina-Esquibel/Ingenieria-datos/blob/main/docs/UT2/extraUT2/UT2Extra.ipynb)

---

## Referencias

- German Credit Dataset â€“ UCI / OpenML  
- Documentation: Fairness in Machine Learning â€“ Google ML Fairness Guide  
- Scikit-learn preprocessing & metrics documentation  
- Intro to AI Ethics â€“ Kaggle  

---

## NavegaciÃ³n

â¬…ï¸ [Volver a Unidad TemÃ¡tica 2](../main.md)  
ğŸ““ [Ãndice del portafolio](../../portfolio/index.md)
