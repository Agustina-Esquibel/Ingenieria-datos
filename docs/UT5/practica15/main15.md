---
title: "Datos en movimiento: creando un pipeline ETL en Google Cloud"
date: 2025-12-07
---

## Contexto
Esta pr√°ctica forma parte de la Unidad Tem√°tica 5, enfocada en **pipelines ETL/ELT modernos** sobre Google Cloud.  
El objetivo del ejercicio es comprender c√≥mo se construye un flujo automatizado de datos donde la extracci√≥n, transformaci√≥n y carga se ejecutan mediante componentes nativos de la nube: **Cloud Storage, Cloud Functions y BigQuery**.

La pr√°ctica introduce los conceptos clave de **data pipelines**, **event-driven processing**, arquitectura desacoplada y procesamiento escalable en la nube.

---

## Objetivos
- Dise√±ar y ejecutar un **pipeline ETL b√°sico** utilizando servicios serverless de Google Cloud.  
- Comprender c√≥mo un archivo que ingresa a Cloud Storage puede **disparar autom√°ticamente** una funci√≥n de transformaci√≥n.  
- Procesar datos mediante una **Cloud Function**, aplicando limpieza, parseo o enriquecimiento.  
- Cargar los datos transformados en **BigQuery**, habilitando consultas posteriores.  
- Reconocer patrones de dise√±o esenciales en pipelines modernos: automatizaci√≥n, monitorizaci√≥n y desacoplamiento.

---

## Desarrollo

### 1. Configuraci√≥n del entorno
Se inici√≥ el laboratorio desde Google Skills Boost con un proyecto temporal de Google Cloud.  
Desde la consola, se verificaron permisos, APIs habilitadas y los recursos disponibles para el ejercicio.

---

### 2. Configuraci√≥n de Cloud Storage
- Se cre√≥ un **bucket de entrada** donde se subir√≠an los archivos fuente.  
- Se configur√≥ el bucket para operar como disparador (trigger) del pipeline.  
- Se subi√≥ un archivo CSV de prueba para iniciar el flujo.

El estudiante comprendi√≥ que Cloud Storage act√∫a como punto de entrada del pipeline ETL.

---

### 3. Implementaci√≥n de la Cloud Function
Se cre√≥ una **Cloud Function con trigger por evento de Storage**, encargada de:

- Leer el archivo entrante.  
- Aplicar transformaciones b√°sicas (parseo, limpieza, normalizaci√≥n o mapping).  
- Generar el dataset de salida compatible con BigQuery.  
- Escribir datos limpios en un nuevo destino o enviarlos directamente al cargador de BigQuery.

La funci√≥n se ejecut√≥ bajo un modelo **serverless**, sin necesidad de administrar servidores.

---

### 4. Carga en BigQuery
Finalmente, los datos procesados fueron enviados a **BigQuery**, donde:

- Se cre√≥ una tabla destino,  
- Se definieron los tipos de datos,  
- Se verific√≥ la carga correcta,  
- Se realizaron consultas simples para validar el funcionamiento del pipeline.

El estudiante comprob√≥ que BigQuery permite almacenar, consultar y analizar datos transformados de forma inmediata.

---

### 5. Ejecuci√≥n del pipeline end-to-end
El pipeline qued√≥ estructurado de esta forma:

1. Un archivo es subido a Cloud Storage.  
2. Cloud Storage dispara una Cloud Function.  
3. La Cloud Function transforma los datos.  
4. Los datos resultantes se cargan autom√°ticamente a BigQuery.

Se ejecut√≥ nuevamente el flujo completo para confirmar que el ETL era **autom√°tico, reproducible y escalable**.

---

## Evidencias

| Evidencia | Descripci√≥n |
|----------|-------------|
| Bucket de entrada creado | Se configur√≥ Cloud Storage como punto de ingreso del pipeline. |
| Cloud Function desplegada | Funci√≥n serverless conectada por evento ‚Äúarchivo subido‚Äù. |
| Transformaci√≥n aplicada | Lectura y limpieza del archivo fuente dentro de la funci√≥n. |
| Carga en BigQuery | Tabla creada y poblada con los datos ya transformados. |
| Pipeline end-to-end | Se valid√≥ el proceso de disparo autom√°tico y carga final. |

---

## Insights clave
- Los pipelines ETL modernos se basan en **automatizaci√≥n**, no en tareas manuales.  
- El uso de triggers convierte a los buckets en **componentes activos del flujo**, no simples contenedores.  
- Cloud Functions permite transformar datos sin administrar servidores, reduciendo complejidad.  
- BigQuery centraliza el almacenamiento anal√≠tico, integr√°ndose naturalmente en el pipeline.  
- La arquitectura obtenida ejemplifica un flujo ETL **desacoplado, escalable y orientado a eventos**.

---

## Reflexi√≥n
La pr√°ctica permiti√≥ comprender c√≥mo los componentes de Google Cloud pueden integrarse para formar un pipeline funcional sin necesidad de infraestructura compleja.  
El estudiante reconoci√≥ la importancia de dise√±ar flujos **reproducibles y mantenibles**, donde cada servicio cumple una responsabilidad clara: almacenar, activar, transformar o cargar datos.  
Este ejercicio sienta las bases para implementar pipelines m√°s avanzados aplicados a escenarios reales de anal√≠tica, data engineering y DataOps.

---

## Referencias
- Google Cloud Skills ‚Äì *"Data in Motion: Building an ETL Pipeline"*  
- Documentaci√≥n oficial de Cloud Functions  
- Documentaci√≥n oficial de Cloud Storage  
- Documentaci√≥n oficial de BigQuery  

---

## Navegaci√≥n
‚¨ÖÔ∏è [Volver a Unidad Tem√°tica 5](../main.md)  

‚û°Ô∏è [DataPrep: limpieza visual de datos orientada a pipeline](../practica16/main16.md)

üìì [√çndice del Portafolio](../../portfolio/index.md)
